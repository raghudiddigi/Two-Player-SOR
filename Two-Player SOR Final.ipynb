{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Code and Value Iteration\n",
    "\n",
    "import numpy as np\n",
    "import nashpy as nash\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    " \n",
    "states = 50\n",
    "actions = 5\n",
    "discount = 0.6\n",
    "max_iterations = 1000 #100000\n",
    "\n",
    "total_avgs = 50\n",
    "\n",
    "P = np.zeros((actions,actions,states,states))\n",
    "R = np.random.random((actions,actions,states))\n",
    "\n",
    "for a1 in range(actions):\n",
    "    for a2 in range(actions):\n",
    "        for s in range(states):\n",
    "            P[a1,a2,s] = np.random.random(states)\n",
    "            P[a1][a2][s][s] = states #check this. \n",
    "            P[a1,a2,s] = P[a1,a2,s] / P[a1,a2,s].sum()\n",
    "            \n",
    "#Value Iteration Starts\n",
    "\n",
    "V = np.zeros(states) #Initial value\n",
    "while True:\n",
    "    Q = np.zeros((actions,actions,states))\n",
    "    for a1 in range(actions):\n",
    "        for a2 in range(actions):\n",
    "            Q[a1,a2] = R[a1,a2] + discount * P[a1,a2].dot(V)\n",
    "\n",
    "    v_prev = deepcopy(V)\n",
    "    #print(v_prev)\n",
    "    for s in range(states):\n",
    "        #print(Q[:,:,s])\n",
    "        rps = nash.Game(Q[:,:,s])\n",
    "        #print(rps)\n",
    "        eqs = rps.lemke_howson(1)\n",
    "        #print(list(eqs))\n",
    "        V[s] = rps[list(eqs)][0]\n",
    "        #print(rps[list(eqs)])\n",
    "\n",
    "    #print(v_prev)\n",
    "    #print(V)\n",
    "    #print(v_prev)\n",
    "    #print(np.linalg.norm(V-v_prev))\n",
    "\n",
    "    if np.linalg.norm(V-v_prev) < 0.000001:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1725467322177435 0.02777552227170664\n",
      "That took 1591.874768257141 seconds\n"
     ]
    }
   ],
   "source": [
    "#Standard minimax Q-Learning\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "store_avgs = np.zeros((total_avgs,1))\n",
    "\n",
    "P_update_time = max_iterations/10\n",
    "for avg in range(total_avgs):\n",
    "    Q = np.random.rand(states,actions,actions)\n",
    "    state = np.random.randint(0, states)\n",
    "    \n",
    "    tot_count = np.zeros((actions,actions,states,states))\n",
    "\n",
    "    w = 1\n",
    "\n",
    "    for n in range(max_iterations):\n",
    "\n",
    "        if (n % 100) == 0:\n",
    "            state = np.random.randint(0, states)\n",
    "\n",
    "        act1 = random.randint(0,actions-1)\n",
    "        act2 = random.randint(0,actions-1)\n",
    "\n",
    "\n",
    "        p_s_new = np.random.random()\n",
    "        p = 0\n",
    "        s_new = -1\n",
    "        while (p < p_s_new) and (s_new < (states - 1)):\n",
    "            s_new = s_new + 1\n",
    "            #print(a1,a2,s,s_new)\n",
    "            p = p + P[act1][act2][state][s_new]\n",
    "\n",
    "        r = R[act1][act2][state]\n",
    "\n",
    "        #print(Q[s_new,:,:])\n",
    "\n",
    "        tot_count[act1][act2][state][s_new] += 1 \n",
    "\n",
    "\n",
    "        rps = nash.Game(Q[state,:,:])\n",
    "        #print(rps)\n",
    "        eqs = rps.lemke_howson(0)\n",
    "        current_state_value = rps[list(eqs)][0]\n",
    "\n",
    "\n",
    "        rps = nash.Game(Q[s_new,:,:])\n",
    "        #print(rps)\n",
    "        eqs = rps.lemke_howson(0)\n",
    "        next_state_value = rps[list(eqs)][0]\n",
    "\n",
    "        delta = r + discount * next_state_value - Q[state, act1,act2]\n",
    "        delta = w *(r + discount* next_state_value) + (1-w)*current_state_value - Q[state, act1,act2]\n",
    "        dQ = (1 / math.sqrt(np.sum(tot_count[act1][act2][state]))) * delta  \n",
    "#         dQ = 0.9 * delta \n",
    "        Q[state, act1,act2] += dQ\n",
    "        state = s_new\n",
    "\n",
    "    sor_minimax_Q = np.zeros(states)\n",
    "    for i in range(states):\n",
    "        rps = nash.Game(Q[i,:,:])\n",
    "        #print(rps)\n",
    "        eqs = rps.lemke_howson(0)\n",
    "#         print(list(eqs))\n",
    "        sor_minimax_Q[i] =  rps[list(eqs)][0]\n",
    "    \n",
    "    store_avgs[avg][0] = np.linalg.norm(V - sor_minimax_Q)\n",
    "#     print(np.linalg.norm(V - sor_minimax_Q))    \n",
    "\n",
    "print(np.average(store_avgs),np.std(store_avgs))    \n",
    "print('That took {} seconds'.format(time.time() - starttime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal w* is 1.58138191702295\n",
      "0.1491782031372383 0.0383542628445545\n",
      "That took 1701.6697669029236 seconds\n"
     ]
    }
   ],
   "source": [
    "#Generalised Optimal minimax Q-Learning\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "store_avgs = np.zeros((total_avgs,1))\n",
    "\n",
    "P_update_time = max_iterations/10\n",
    "store_w = np.zeros((max_iterations,total_avgs))\n",
    "\n",
    "w = 100\n",
    "        \n",
    "for a1 in range(actions):\n",
    "    for a2 in range(actions):\n",
    "        for s in range(states):\n",
    "            temp = 1/(1 - (discount*P[a1][a2][s][s]))\n",
    "            if w > temp:\n",
    "                w = temp\n",
    "\n",
    "print(\"optimal w* is\",w)\n",
    "\n",
    "\n",
    "for avg in range(total_avgs):\n",
    "    Q = np.random.rand(states,actions,actions)\n",
    "    state = np.random.randint(0, states)\n",
    "    \n",
    "\n",
    "    tot_count = np.zeros((actions,actions,states,states))\n",
    "    \n",
    "    for n in range(max_iterations):\n",
    "\n",
    "        if (n % 100) == 0:\n",
    "            state = np.random.randint(0, states)\n",
    "\n",
    "        act1 = random.randint(0,actions-1)\n",
    "        act2 = random.randint(0,actions-1)\n",
    "\n",
    "\n",
    "        p_s_new = np.random.random()\n",
    "        p = 0\n",
    "        s_new = -1\n",
    "        while (p < p_s_new) and (s_new < (states - 1)):\n",
    "            s_new = s_new + 1\n",
    "            #print(a1,a2,s,s_new)\n",
    "            p = p + P[act1][act2][state][s_new]\n",
    "\n",
    "        r = R[act1][act2][state]\n",
    "\n",
    "        #print(Q[s_new,:,:])\n",
    "\n",
    "        tot_count[act1][act2][state][s_new] += 1 \n",
    "\n",
    "        store_w[n][avg] = w\n",
    "\n",
    "        rps = nash.Game(Q[state,:,:])\n",
    "        #print(rps)\n",
    "        eqs = rps.lemke_howson(0)\n",
    "        current_state_value = rps[list(eqs)][0]\n",
    "\n",
    "\n",
    "        rps = nash.Game(Q[s_new,:,:])\n",
    "        #print(rps)\n",
    "        eqs = rps.lemke_howson(0)\n",
    "        next_state_value = rps[list(eqs)][0]\n",
    "\n",
    "        delta = r + discount * next_state_value - Q[state, act1,act2]\n",
    "        delta = w *(r + discount* next_state_value) + (1-w)*current_state_value - Q[state, act1,act2]\n",
    "        dQ = (1 / math.sqrt(np.sum(tot_count[act1][act2][state]))) * delta  \n",
    "#         dQ = 0.9 * delta \n",
    "        Q[state, act1,act2] += dQ\n",
    "        state = s_new\n",
    "\n",
    "    sor_minimax_Q = np.zeros(states)\n",
    "    for i in range(states):\n",
    "        rps = nash.Game(Q[i,:,:])\n",
    "        #print(rps)\n",
    "        eqs = rps.lemke_howson(0)\n",
    "#         print(list(eqs))\n",
    "        sor_minimax_Q[i] =  rps[list(eqs)][0]\n",
    "    \n",
    "    store_avgs[avg][0] = np.linalg.norm(V - sor_minimax_Q)\n",
    "#     print(np.linalg.norm(V - sor_minimax_Q))    \n",
    "\n",
    "print(np.average(store_avgs),np.std(store_avgs))     \n",
    "print('That took {} seconds'.format(time.time() - starttime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14982317491203284 0.04414921895663536\n",
      "That took 4700.354353189468 seconds\n"
     ]
    }
   ],
   "source": [
    "#Generalised Minimax Q-Learning\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "store_avgs = np.zeros((total_avgs,1))\n",
    "\n",
    "P_update_time = max_iterations/10\n",
    "store_w = np.zeros((max_iterations,total_avgs))\n",
    "for avg in range(total_avgs):\n",
    "    Q = np.random.rand(states,actions,actions)\n",
    "    state = np.random.randint(0, states)\n",
    "    \n",
    "\n",
    "    tot_count = np.zeros((actions,actions,states,states))\n",
    "\n",
    "    w = 1\n",
    "    \n",
    "    for n in range(max_iterations):\n",
    "\n",
    "        if (n % 100) == 0:\n",
    "            state = np.random.randint(0, states)\n",
    "\n",
    "        act1 = random.randint(0,actions-1)\n",
    "        act2 = random.randint(0,actions-1)\n",
    "\n",
    "\n",
    "        p_s_new = np.random.random()\n",
    "        p = 0\n",
    "        s_new = -1\n",
    "        while (p < p_s_new) and (s_new < (states - 1)):\n",
    "            s_new = s_new + 1\n",
    "            #print(a1,a2,s,s_new)\n",
    "            p = p + P[act1][act2][state][s_new]\n",
    "\n",
    "        r = R[act1][act2][state]\n",
    "\n",
    "        #print(Q[s_new,:,:])\n",
    "\n",
    "        tot_count[act1][act2][state][s_new] += 1 \n",
    "\n",
    "#         if n > P_update_time:  #Doing it after initial 1000 iterations\n",
    "        new_w = 1/(1-discount)\n",
    "        for a1 in range(actions):\n",
    "            for a2 in range(actions):\n",
    "                for s in range(states):\n",
    "                    if np.sum(tot_count[a1][a2][s][s]) > 0:\n",
    "                        temp = 1/(1 - (discount*(tot_count[a1][a2][s][s]/np.sum(tot_count[a1][a2][s]))))\n",
    "                        #print(temp)\n",
    "                        if new_w > temp:\n",
    "                            new_w = temp\n",
    "                            \n",
    "        step_size = 1 / math.sqrt(n+1)\n",
    "        w = (1-step_size)*w+step_size*new_w\n",
    "\n",
    "\n",
    "        store_w[n][avg] = w\n",
    "\n",
    "        rps = nash.Game(Q[state,:,:])\n",
    "        #print(rps)\n",
    "        eqs = rps.lemke_howson(0)\n",
    "        current_state_value = rps[list(eqs)][0]\n",
    "\n",
    "\n",
    "        rps = nash.Game(Q[s_new,:,:])\n",
    "        #print(rps)\n",
    "        eqs = rps.lemke_howson(0)\n",
    "        next_state_value = rps[list(eqs)][0]\n",
    "\n",
    "        delta = r + discount * next_state_value - Q[state, act1,act2]\n",
    "        delta = w *(r + discount* next_state_value) + (1-w)*current_state_value - Q[state, act1,act2]\n",
    "        dQ = (1 / math.sqrt(np.sum(tot_count[act1][act2][state]))) * delta\n",
    "        #         dQ = 0.9 * delta \n",
    "        Q[state, act1,act2] += dQ\n",
    "        state = s_new\n",
    "\n",
    "    sor_minimax_Q = np.zeros(states)\n",
    "    for i in range(states):\n",
    "        rps = nash.Game(Q[i,:,:])\n",
    "        #print(rps)\n",
    "        eqs = rps.lemke_howson(0)\n",
    "#         print(list(eqs))\n",
    "        sor_minimax_Q[i] =  rps[list(eqs)][0]\n",
    "    \n",
    "    store_avgs[avg][0] = np.linalg.norm(V - sor_minimax_Q)\n",
    "#     print(np.linalg.norm(V - sor_minimax_Q))    \n",
    "\n",
    "print(np.average(store_avgs),np.std(store_avgs))     \n",
    "print('That took {} seconds'.format(time.time() - starttime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4764f45828>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.average(store_w,axis = 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
