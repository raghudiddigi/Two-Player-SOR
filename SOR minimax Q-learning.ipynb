{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(states,actions,discount,P,R):\n",
    "    V = np.zeros(states) #Initial value\n",
    "    iterations = 1000    \n",
    "        \n",
    "    for iters in range(iterations):\n",
    "        Q = np.zeros((actions,actions,states))\n",
    "        for a1 in range(actions):\n",
    "            for a2 in range(actions):\n",
    "                Q[a1,a2] = R[a1,a2] + discount * P[a1,a2].dot(V)\n",
    "\n",
    "        v_prev = deepcopy(V)\n",
    "        #print(v_prev)\n",
    "        for s in range(states):\n",
    "            #print(Q[:,:,s])\n",
    "            rps = nash.Game(Q[:,:,s])\n",
    "            #print(rps)\n",
    "            eqs = rps.lemke_howson(0)\n",
    "            #print(list(eqs))\n",
    "            V[s] = rps[list(eqs)][0]\n",
    "            #print(rps[list(eqs)])\n",
    "\n",
    "        #print(v_prev)\n",
    "        #print(V)\n",
    "        #print(v_prev)\n",
    "        #print(np.linalg.norm(V-v_prev))\n",
    "\n",
    "        if np.linalg.norm(V-v_prev) < 0.00001:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax_Q(states,actions,discount,P,R,V,max_iterations):\n",
    "    \n",
    "    np.random.seed(100)\n",
    "    \n",
    "    Q = np.random.rand(states,actions,actions)\n",
    "    s = np.random.randint(0, states)\n",
    "    standard_norm_diff = np.zeros((max_iterations,1))\n",
    "\n",
    "    for n in range(max_iterations):\n",
    "\n",
    "        if (n % 100) == 0:\n",
    "            s = np.random.randint(0, states)\n",
    "\n",
    "        a1 = random.randint(0,actions-1)\n",
    "        a2 = random.randint(0,actions-1)\n",
    "\n",
    "\n",
    "        p_s_new = np.random.random()\n",
    "        p = 0\n",
    "        s_new = -1\n",
    "        while (p < p_s_new) and (s_new < (states - 1)):\n",
    "            s_new = s_new + 1\n",
    "            #print(a1,a2,s,s_new)\n",
    "            p = p + P[a1][a2][s][s_new]\n",
    "\n",
    "        r = R[a1][a2][s]\n",
    "\n",
    "        rps = nash.Game(Q[s_new,:,:])\n",
    "        #print(rps)\n",
    "        eqs = rps.lemke_howson(0)\n",
    "        next_state_value = rps[list(eqs)][0]\n",
    "\n",
    "        delta = r + discount * next_state_value - Q[s, a1,a2]\n",
    "        dQ = (1 / math.sqrt(n + 2)) * delta\n",
    "                #dQ = 0.001*delta\n",
    "        Q[s, a1,a2] = Q[s, a1,a2] + dQ\n",
    "\n",
    "        minimax_Q = np.zeros(states)\n",
    "        for i in range(states):\n",
    "            rps = nash.Game(Q[i,:,:])\n",
    "            #print(rps)\n",
    "            eqs = rps.lemke_howson(0)\n",
    "            minimax_Q[i] = (rps[list(eqs)][0])\n",
    "\n",
    "        standard_norm_diff[n] = np.linalg.norm(V - minimax_Q)\n",
    "\n",
    "    return standard_norm_diff,np.linalg.norm(V - minimax_Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax_SOR_Q(states,actions,discount,w,P,R,V,max_iterations):\n",
    "    \n",
    "    np.random.seed(100)\n",
    "    \n",
    "    #SOR Q-learning\n",
    "    Q = np.random.rand(states,actions,actions)\n",
    "    s = np.random.randint(0, states)\n",
    "    sor_norm_diff = np.zeros((max_iterations,1))\n",
    "    \n",
    "\n",
    "    for n in range(max_iterations):\n",
    "\n",
    "        if (n % 100) == 0:\n",
    "            s = np.random.randint(0, states)\n",
    "\n",
    "        a1 = random.randint(0,actions-1)\n",
    "        a2 = random.randint(0,actions-1)\n",
    "\n",
    "\n",
    "        p_s_new = np.random.random()\n",
    "        p = 0\n",
    "        s_new = -1\n",
    "        while (p < p_s_new) and (s_new < (states - 1)):\n",
    "            s_new = s_new + 1\n",
    "            #print(a1,a2,s,s_new)\n",
    "            p = p + P[a1][a2][s][s_new]\n",
    "\n",
    "        r = R[a1][a2][s]\n",
    "\n",
    "        #print(Q[s_new,:,:])\n",
    "\n",
    "\n",
    "        rps = nash.Game(Q[s,:,:])\n",
    "        #print(rps)\n",
    "        eqs = rps.lemke_howson(0)\n",
    "        current_state_value = rps[list(eqs)][0]\n",
    "\n",
    "\n",
    "        rps = nash.Game(Q[s_new,:,:])\n",
    "        #print(rps)\n",
    "        eqs = rps.lemke_howson(0)\n",
    "        next_state_value = rps[list(eqs)][0]\n",
    "\n",
    "        delta = r + discount * next_state_value - Q[s, a1,a2]\n",
    "        delta = w *(r + discount* next_state_value) + (1-w)*current_state_value - Q[s, a1,a2]\n",
    "        dQ = (1 / math.sqrt(n + 2)) * delta\n",
    "                #dQ = 0.001*delta\n",
    "        Q[s, a1,a2] = Q[s, a1,a2] + dQ\n",
    "\n",
    "        sor_minimax_Q = np.zeros(states)\n",
    "        \n",
    "        for i in range(states):\n",
    "            rps = nash.Game(Q[i,:,:])\n",
    "            #print(rps)\n",
    "            eqs = rps.lemke_howson(0)\n",
    "            sor_minimax_Q[i] =  rps[list(eqs)][0]\n",
    "        \n",
    "        sor_norm_diff[n] = np.linalg.norm(V - sor_minimax_Q) \n",
    "        \n",
    "\n",
    "    return sor_norm_diff, np.linalg.norm(V - sor_minimax_Q)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_run(count,standard_diffs,sor_diffs):\n",
    "\n",
    "    np.random.seed((count+1)*100)\n",
    "    random.seed((count+1)*110)\n",
    "\n",
    "    P = np.zeros((actions,actions,states,states))\n",
    "    R = np.random.random((actions,actions,states))\n",
    "    \n",
    "    const = np.random.randint(2,6) #For esnuring positive probability and w>1. \n",
    "\n",
    "    for a1 in range(actions):\n",
    "        for a2 in range(actions):\n",
    "            for s in range(states):\n",
    "                P[a1,a2,s] = np.random.random(states)\n",
    "                P[a1][a2][s][s] = const\n",
    "                P[a1,a2,s] = P[a1,a2,s] / P[a1,a2,s].sum()\n",
    "                \n",
    "    \n",
    "    w = 100\n",
    "        \n",
    "    for a1 in range(actions):\n",
    "        for a2 in range(actions):\n",
    "            for s in range(states):\n",
    "                temp = 1/(1 - (discount*P[a1][a2][s][s]))\n",
    "                if w > temp:\n",
    "                    w = temp\n",
    "    #print(w)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    V = value_iteration(states,actions,discount,P,R)\n",
    "    standard_diffs[count], standard_last_diff = minimax_Q(states,actions,discount,P,R,V,max_iterations)\n",
    "    \n",
    "#     sor_diffs_a[count], sor_last_diff_a = minimax_SOR_Q(states,actions,discount,1.1,P,R,V,max_iterations)\n",
    "#     sor_diffs_b[count], sor_last_diff_b = minimax_SOR_Q(states,actions,discount,1.2,P,R,V,max_iterations)\n",
    "#     sor_diffs_c[count], sor_last_diff_c = minimax_SOR_Q(states,actions,discount,1.3,P,R,V,max_iterations)\n",
    "\n",
    "    sor_diffs[count], sor_last_diff = minimax_SOR_Q(states,actions,discount,w,P,R,V,max_iterations)\n",
    "    print(standard_last_diff,sor_last_diff)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nashpy as nash\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import multiprocessing \n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "standard_diffs = manager.dict()\n",
    "sor_diffs = manager.dict()\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "total_mdps = 50\n",
    "\n",
    "states = 10\n",
    "actions = 5\n",
    "discount = 0.6\n",
    "max_iterations = 100000\n",
    "\n",
    "# standard_diffs = np.zeros((total_mdps,max_iterations,1))\n",
    "# sor_diffs = np.zeros((total_mdps,max_iterations,1))\n",
    "\n",
    "\n",
    "\n",
    "# sor_diffs_a = np.zeros((total_mdps,max_iterations))\n",
    "# sor_diffs_b = np.zeros((total_mdps,max_iterations))\n",
    "# sor_diffs_c = np.zeros((total_mdps,max_iterations))\n",
    "# sor_diffs_d = np.zeros((total_mdps,max_iterations))\n",
    "\n",
    "processes = []\n",
    "\n",
    "for count in range(total_mdps):\n",
    "    \n",
    "    p = multiprocessing.Process(target=code_run, args=(count,standard_diffs,sor_diffs))\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "\n",
    "for process in processes:\n",
    "        process.join()\n",
    "    \n",
    "    \n",
    "np.savetxt('minmax-normal',np.average(standard_diffs.values(),axis = 0))\n",
    "np.savetxt('minmax-sor',np.average(sor_diffs.values(),axis = 0))\n",
    "\n",
    "print('That took {} seconds'.format(time.time() - starttime))\n",
    "            \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
